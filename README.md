# Statisics and Data Science for Engineers At University of California Berkeley

In this course you will learn how to use data to build models of systems. We will start by establishing a theoretical foundation of basic concepts from probability theory (events, distributions, conditional probability, Bayes' theorem, etc) and optimization theory (local vs. global solutions, first order necessary conditions, convexity, gradient descent). With this in place, we will then learn some important techniques from statistics for input-less systems: point estimation, maximum likelihood, confidence intervals, and others. These techniques are important and useful in their own right, but they also serve as an introduction to the main topic: models of static and dynamical systems with inputs. In each case we will begin with classical techniques, rooted in statistics, such as linear and logistic regression for static systems, and ARMA for dynamical systems. Finally we will engage the more recent "machine learning" techniques for these problems: support vector machines, ensemble methods such as random forests, and neural networks.

# List of topics
+ Probability theory
  + Sample space, pdfs, expectation, variance
  + Conditional probability, Bayes' rule, independence, correlation
  + Parametric pdfs
  + Central limit theorem. 
+ Optimization theory
  + Problem formulation and solution types
  + Convexity
  + Gradient descent
+ Statistics
  + Point estimation
  + Maximum likelihood
  + Confidence intervals, hypothesis tests
  + Mixture Gaussian models, K-means clustering
+ Supervised learning
  + Problem formulation, loss functions
  + Assessing performance, cross-validation
  + Hyper-parameters
+ Linear regression
  + Simple and general cases
  + Uncertainty in the parameters and prediction uncertainty
  + Best subset selection
  + Regularization
+ Classification
  + Problem formulation, K nearest neighbors
  + Naive Bayes
+ Logistic Regression
  + Simple and general cases
  + Cross entropy loss
  + Multinomial logistic regression
+ Time-series data
  + Problem formulation
  + ARMA
  + Non-linear approaches
+ Neural networks
  + From linear regression to ANNs
  + Classification networks
  + Training neural networks
  + CNNs
+ Decision trees and ensemble methods
  + Decision trees
  + Ensemble bagging, random forests
  + Ensemble boosting, gradient boosting, Adaboost
 
# Staff
+ Lecturer: Gabriel Gomez (gomes@berkeley.edu)
+ GSIs:
  + Catherine Weaver
  + Ian Li
+ Readers:
  + Yishu Yan
  + Hyeong Yoon
