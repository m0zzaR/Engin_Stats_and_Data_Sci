# Statistics and Data Science for Engineers (E178)
**University of California, Berkeley**

## Course Overview
In this course, you will learn how to use data to build models of systems. We will start by establishing a theoretical foundation of basic concepts from probability theory (events, distributions, conditional probability, Bayes' theorem, etc.) and optimization theory (local vs. global solutions, first-order necessary conditions, convexity, gradient descent).

With this foundation, we will then learn important techniques from statistics for input-less systems, such as point estimation, maximum likelihood, and confidence intervals. These techniques introduce the main topic: models of static and dynamical systems with inputs. We will cover classical techniques like linear and logistic regression for static systems and ARMA for dynamical systems. Finally, we will engage with more recent "machine learning" techniques like support vector machines, ensemble methods (e.g., random forests), and neural networks.

## List of Topics

### Probability Theory
- Sample space, PDFs, expectation, variance
- Conditional probability, Bayes' rule, independence, correlation
- Parametric PDFs
- Central Limit Theorem

### Optimization Theory
- Problem formulation and solution types
- Convexity
- Gradient descent

### Statistics
- Point estimation
- Maximum likelihood
- Confidence intervals, hypothesis tests
- Mixture Gaussian models, K-means clustering

### Supervised Learning
- Problem formulation, loss functions
- Assessing performance, cross-validation
- Hyper-parameters

### Linear Regression
- Simple and general cases
- Uncertainty in the parameters and prediction uncertainty
- Best subset selection
- Regularization

### Classification
- Problem formulation, K-nearest neighbors
- Naive Bayes

### Logistic Regression
- Simple and general cases
- Cross-entropy loss
- Multinomial logistic regression

### Time-Series Data
- Problem formulation
- ARMA
- Non-linear approaches

### Neural Networks
- From linear regression to ANNs
- Classification networks
- Training neural networks
- CNNs (Convolutional Neural Networks)

### Decision Trees and Ensemble Methods
- Decision trees
- Ensemble bagging, random forests
- Ensemble boosting, gradient boosting, Adaboost

## Staff

- **Lecturer:** Gabriel Gomez ([gomes@berkeley.edu](mailto:gomes@berkeley.edu))
- **GSIs:**
  - Catherine Weaver
  - Ian Li
- **Readers:**
  - Yishu Yan
  - Hyeong Yoon

