{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756513a4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab5.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ad917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:38.684988Z",
     "iopub.status.busy": "2023-10-27T21:55:38.684606Z",
     "iopub.status.idle": "2023-10-27T21:55:39.193524Z",
     "shell.execute_reply": "2023-10-27T21:55:39.192961Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "rng_seed = 454"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1877b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1><center>SDSE Lab 5 <br><br> Scikit-learn, logistic regression, feature selection, and regularization</center></h1>\n",
    "\n",
    "---\n",
    "\n",
    "In this lab we will build a model for diagnosing breast cancer from various measurements of a tumor. To do this we will use [scikit-learn](https://scikit-learn.org/stable/), which is a package for performing a host of machine learning tasks. We will learn about scikit-learn's train-test data splitter, its standard scaler, pipelines, cross-validation, and LASSO regularization. \n",
    "\n",
    "The lab has 12 parts across four sections.\n",
    "\n",
    "**Prelminaries**\n",
    "\n",
    "1. Load the data\n",
    "2. Extract test data\n",
    "3. Normalize the training data\n",
    "\n",
    "**Simple logistic regression**\n",
    "\n",
    "4. Most correlated feature\n",
    "5. Train simple logistic regression with normalized and unnormalized inputs\n",
    "6. Create a scikit-learn pipeline\n",
    "7. Evaluate the models with cross-validation\n",
    "8. Evaluate the models with test data\n",
    "\n",
    "**Regularization**\n",
    "\n",
    "9. LASSO regularized logistic regression\n",
    "10. Choose the best model\n",
    "11. Significant features\n",
    "12. Evaluate the final model with test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d138ec",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<h1><center><font color='purple'> Preliminaries</font><br></center></h1>\n",
    "\n",
    "\n",
    "# 1. Load the data\n",
    "\n",
    "This is a [classic dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)) that originates from the University of Wisconsin and is included in the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), as well as in scikit-learn's collection of [toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html). It can be loaded with the [load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) method. Passing `as_frame=True` prompts the loader to return a pandas DataFrame. \n",
    "\n",
    "This dataset encodes a benign tumor as a 1 and a malignant tumor as a 0. We flip these tags so that the encoding agrees with our common notion of \"positive\" and \"negative\" diagnoses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd359f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:39.196108Z",
     "iopub.status.busy": "2023-10-27T21:55:39.195855Z",
     "iopub.status.idle": "2023-10-27T21:55:39.500861Z",
     "shell.execute_reply": "2023-10-27T21:55:39.500300Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer(as_frame=True).frame\n",
    "data['target'] = 1-data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054c967a",
   "metadata": {},
   "source": [
    "Use `data.info()` to display a summary of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3cc268",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:39.503397Z",
     "iopub.status.busy": "2023-10-27T21:55:39.503135Z",
     "iopub.status.idle": "2023-10-27T21:55:39.511015Z",
     "shell.execute_reply": "2023-10-27T21:55:39.510585Z"
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947bd93",
   "metadata": {},
   "source": [
    "# 2. Extract test data\n",
    "\n",
    "The first step is to set aside a portion of the data for final testing. Use scikit-learn's [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to create the testing and training datasets. \n",
    "\n",
    "Note: `train_test_split` takes these arguments:\n",
    "1. The input samples: Use `data.iloc` to select all rows and all but the last column. \n",
    "2. The target (output) samples: The last column of `data` (named \"target\")\n",
    "3. `test_size` is the portion of the dataset reserved for testing. You should set this to 20% (0.2).\n",
    "4. Pass `random_state=rng_seed` to fix the random seed and ensure reproducibility of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e93b27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:39.513130Z",
     "iopub.status.busy": "2023-10-27T21:55:39.512929Z",
     "iopub.status.idle": "2023-10-27T21:55:39.537547Z",
     "shell.execute_reply": "2023-10-27T21:55:39.537097Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(...,      # TODO\n",
    "                                                ...,      # TODO\n",
    "                                                test_size=...,       # TODO\n",
    "                                                random_state=rng_seed )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd84fb27",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d68f9f",
   "metadata": {},
   "source": [
    "# 3. Normalize the training data\n",
    "\n",
    "Next we will normalize the data, as we have done before, by subtracting its mean and dividing each column by its standard deviation. This is not strictly necessary for un-regularized logistic regression from a theoretical viewpoint. The algorithm is the same with normalized and un-normalized data. However it can have beneficial effects on the numerical robustness of the optimization solver. \n",
    "\n",
    "We use scikit-learn's [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to perform the normalization on the training input data (`Xtrain`). Then we put the result into a new pandas DataFrame.\n",
    "\n",
    "**Hint** How can you get the index and column names of a pandas DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4f08d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:39.550033Z",
     "iopub.status.busy": "2023-10-27T21:55:39.549865Z",
     "iopub.status.idle": "2023-10-27T21:55:39.607624Z",
     "shell.execute_reply": "2023-10-27T21:55:39.607159Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(Xtrain)\n",
    "Xtrain_norm = pd.DataFrame(X, index=..., columns=...)    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab24103",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa87805c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<h1><center><font color='purple'> Simple logistic regression</font><br></center></h1>\n",
    "\n",
    "\n",
    "# 4. Most correlated feature\n",
    "\n",
    "Our first model will be a simple logistic regression model based on the single feature that best correlates with the output. Find this feature and save its name (i.e. its header value) to `best_single_feature`. \n",
    "\n",
    "Note: The tests for this part are hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f72692",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:39.621132Z",
     "iopub.status.busy": "2023-10-27T21:55:39.620947Z",
     "iopub.status.idle": "2023-10-27T21:55:39.626445Z",
     "shell.execute_reply": "2023-10-27T21:55:39.625959Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "...         # TODO\n",
    "best_single_feature = ...   # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac963577",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f08a4",
   "metadata": {},
   "source": [
    "# 5. Train simple logistic regression\n",
    "\n",
    "Next we train the simple logistic regression model for the feature that was selected in the previous part. We will use scikit-learn's implementation of [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for this purpose. \n",
    "\n",
    "1. Pass `random_state=rng_seed` into the LogisticRegression constructor to ensure repeatability of the results. \n",
    "2. Call the [`fit`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit) function of the model object, passing in the training data. The model inputs correspond to the single best feature already identified.\n",
    "3. Extract the trained model coefficients. The intercept term $\\hat\\theta_0$ is contained in the `intercept_[0]` attribute of the model. The remaining coefficients $\\hat\\theta_1$ through $\\hat\\theta_P$ (in this case just $\\hat\\theta_1$) are in `coef_[0,:]`.\n",
    "\n",
    "This has been done for you with the original (un-normalized) input data. Repeat the exercise with the normalized data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457ec6db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:39.633803Z",
     "iopub.status.busy": "2023-10-27T21:55:39.633619Z",
     "iopub.status.idle": "2023-10-27T21:55:39.665053Z",
     "shell.execute_reply": "2023-10-27T21:55:39.664593Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_nonorm = LogisticRegression(random_state=rng_seed)\n",
    "model_nonorm.fit(Xtrain[[best_single_feature]],ytrain) \n",
    "print(model_nonorm.intercept_[0], model_nonorm.coef_[0,:])\n",
    "\n",
    "model_norm = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e411227",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528accb",
   "metadata": {},
   "source": [
    "# 6. Create a scikit-learn pipeline\n",
    "\n",
    "Scikit-learn provides a *pipeline* class that collects all of the preprocessing, feature transformation, and modeling components into a single object with `fit` and `predict` methods. You can  read the documentation on [pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to learn more. \n",
    "\n",
    "Each component in the pipeline is identified with a string name. The following code creates a pipeline with a `StandardScaler` tagged as `scaler`, followed by a logistic regression model tagged as `logreg`.\n",
    "\n",
    "``` python\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('logreg', LogisticRegression(random_state=rng_seed)) ])\n",
    "```\n",
    "\n",
    "Create this pipeline and train it on the `best_single_feature` of the un-normalized dataset (`Xtrain`,`ytrain`) using the `fit` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25c053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:39.678513Z",
     "iopub.status.busy": "2023-10-27T21:55:39.678346Z",
     "iopub.status.idle": "2023-10-27T21:55:39.688782Z",
     "shell.execute_reply": "2023-10-27T21:55:39.688363Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(...)\n",
    "pipeline.fit(...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5196913",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a32a7e",
   "metadata": {},
   "source": [
    "# 7. Evaluate the models with cross-validation\n",
    "\n",
    "Accuracy is an important performance metric for classification models. It is computed as the ratio of correct predictions to the total number of predictions. Hence it approximates the probability that the prediction is correct. \n",
    "\n",
    "K-fold cross-validation is an evaluation technique that provides a robust estimate of model performance (e.g. accuracy) without the need for test data. It does this by splitting the training set into K equal parts (or \"folds\"), and then training K separate models, each with one of the K parts as validation data and the others as training data. \n",
    "\n",
    "Cross-validation is implemented in scikit-learn's [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function. We will use 3-fold cross-validation to evaluate the accuracy of our three models: `model_nonorm`, `model_norm`, and `pipeline`.\n",
    "\n",
    "Note the following:\n",
    "1. The first three arguments for the `cross_val_score` are the model, the training input data, and the training output data. These last two entries are the same as were passed to the `fit` function in the previous part. \n",
    "2. Use `scoring='accuracy'` to set the evaluation metric to accuracy. Use `cv=3` to set the number of folds to 3. \n",
    "3. The function should return 3 values of accuracy -- one for each of the folds. Store the *mean* of these as `acc_nonorm`, `acc_norm`, and `acc_pipe` for the un-normalized, normalized, and pipeline models respectively. \n",
    "4. Note the improvement due to normalization. What do you think might account for the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea879b90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:39.707488Z",
     "iopub.status.busy": "2023-10-27T21:55:39.707314Z",
     "iopub.status.idle": "2023-10-27T21:55:39.763751Z",
     "shell.execute_reply": "2023-10-27T21:55:39.763156Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "acc_nonorm = ...      #  TODO\n",
    "acc_norm = ...      #  TODO\n",
    "acc_pipe = ...      #  TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b2e64",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b52bb6",
   "metadata": {},
   "source": [
    "# 8. Evaluate the models with test data\n",
    "\n",
    "We can also use the test data to evaluate the performance of our three models. Here we will use the [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) method to compute the test accuracy in each case. \n",
    "\n",
    "`accuracy_score` takes as inputs the true and the predicted values. Use the `predict` function of the model to obtain the predicted output. Then pass the predictions along with the true values of the output into `accuracy_score`.\n",
    "Do this for the non-normalized model (`model_nonorm`) and for the pipeline model (`pipeline`). Save the results to `acc_nonorm_test` and `acc_pipe_test` respectively. How do these values compare to the accuracies obtained by cross-validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562872eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:39.783012Z",
     "iopub.status.busy": "2023-10-27T21:55:39.782758Z",
     "iopub.status.idle": "2023-10-27T21:55:39.790704Z",
     "shell.execute_reply": "2023-10-27T21:55:39.790190Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "yhat_nonorm = model_nonorm.predict(...)\n",
    "acc_nonorm_test = accuracy_score(..., ...)\n",
    "\n",
    "yhat_pipe = pipeline.predict(...)\n",
    "acc_pipe_test = accuracy_score(..., ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc1338",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c76a25e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<h1><center><font color='purple'>Regularization</font><br></center></h1>\n",
    "\n",
    "\n",
    "# 9. LASSO regularized logistic regression\n",
    "\n",
    "Regularization is a method for avoiding overfitting by penalizing the complexity of the model in the training process. LASSO regularization in particular penalizes the sum of the absolute values of the parameters. It has the property that it will tend to \"zero out\" coefficients as the penalty $\\lambda$ increases. This gives it an additional use as a feature selector. \n",
    "\n",
    "In this part we will train a LASSO regularized logistic regression model. Instead of $\\lambda$, we will use the `C` parameter of `LogisticRegression`, which is the inverse of $\\lambda$. \n",
    "\n",
    "The code iterates through a logarithmically spaced array of regularization parameters `C`. For each value it trains and evaluates a logistic regression pipeline with the regularization parameter set to that value. \n",
    "\n",
    "Your task is to complete the code. Your pipeline should have two componenents: a `StandardScaler` for normalizing the data, followed by a `LogisticRegression` regression model. When building the pipeline, you should pass these parameters to the `LogisticRegression` constructor: \n",
    "\n",
    "```python \n",
    "C=C[c],\n",
    "penalty='l1',\n",
    "solver='liblinear',\n",
    "```\n",
    "\n",
    "in addition to setting the random state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a7190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:39.804008Z",
     "iopub.status.busy": "2023-10-27T21:55:39.803687Z",
     "iopub.status.idle": "2023-10-27T21:55:40.648976Z",
     "shell.execute_reply": "2023-10-27T21:55:40.648418Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "C = np.logspace(-2,2,20)\n",
    "acc = np.empty(20)\n",
    "models = list()\n",
    "\n",
    "for c in range(len(C)):   \n",
    "    \n",
    "    print(c)\n",
    "    \n",
    "    model = Pipeline([...\n",
    "                      ... ])\n",
    "    model.fit(...,...)\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "    # Validation accuracy\n",
    "    acc[c] = cross_val_score(model, Xtrain, ytrain, cv=3, scoring='accuracy').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d53cb8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95982218",
   "metadata": {},
   "source": [
    "# 10. Choose the best model\n",
    "\n",
    "Next we select the model with the best validation accuracy. Follow the steps in the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c300cee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:40.658035Z",
     "iopub.status.busy": "2023-10-27T21:55:40.657860Z",
     "iopub.status.idle": "2023-10-27T21:55:41.460194Z",
     "shell.execute_reply": "2023-10-27T21:55:41.459555Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Set `cstar` to the index of the best performing regularization value\n",
    "cstar = ...\n",
    "\n",
    "# 2. Set `acc_star` to the corresponding accuracy value\n",
    "acc_star = ...\n",
    "\n",
    "# The next bit of code extracts the coefficients of the logistic regression for each of the 20 values of `C`. \n",
    "# This is stored in `theta` , which is a (20,30) array. (30 is the number of features)\n",
    "theta = np.vstack([model.named_steps['logreg'].coef_[0,:] for model in models])\n",
    "\n",
    "# 3. Plot the validation accuracy as a function of `C`. (done already)\n",
    "fig, ax = plt.subplots(figsize=(8,8),nrows=2,sharex=True)\n",
    "ax[0].semilogx(C,acc,'o-',color='b',linewidth=2)\n",
    "ax[0].semilogx(C[cstar],acc_star,'*',color='b',markersize=14)\n",
    "ax[0].grid(linestyle=':')\n",
    "ax[0].set_ylabel('validation accuracy',fontsize=12)\n",
    "\n",
    "# 4. In a single plot, plot the 30 coefficients as a fucntion of `C`.\n",
    "ax[1].semilogx(C,theta)\n",
    "ax[1].grid(linestyle=':')\n",
    "ax[1].set_xlabel('C',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc3af7b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6afc073",
   "metadata": {},
   "source": [
    "# 11. Significant features\n",
    "\n",
    "The plot below shows the coefficients for the best-case regularized logistic regression found in the previous part. Notice that many of these coefficients have been set to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5865f2eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:41.474689Z",
     "iopub.status.busy": "2023-10-27T21:55:41.474515Z",
     "iopub.status.idle": "2023-10-27T21:55:41.599619Z",
     "shell.execute_reply": "2023-10-27T21:55:41.598974Z"
    }
   },
   "outputs": [],
   "source": [
    "theta_star = theta[cstar,:]\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.stem(np.abs(theta_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd08f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:41.602167Z",
     "iopub.status.busy": "2023-10-27T21:55:41.601959Z",
     "iopub.status.idle": "2023-10-27T21:55:41.606615Z",
     "shell.execute_reply": "2023-10-27T21:55:41.606160Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = Xtrain.columns\n",
    "\n",
    "# 1. Set `best_features` to the set of feature names corresponding to non-zero coefficients in the plot above. \n",
    "best_features = ...\n",
    "\n",
    "# 2. Set `max_theta_feature` to the feature name corresponding to the coefficient with maximum absolute value. \n",
    "max_theta_feature = ...\n",
    "\n",
    "# 3. Save the selected lasso model to the variable `lasso_model`.\n",
    "lasso_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897d93e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e793047",
   "metadata": {},
   "source": [
    "# 12. Evaluate the final model with test data\n",
    "\n",
    "Use the test dataset to evaluate the accuracy of the selected LASSO model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f23131",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-27T21:55:41.626043Z",
     "iopub.status.busy": "2023-10-27T21:55:41.625865Z",
     "iopub.status.idle": "2023-10-27T21:55:41.631557Z",
     "shell.execute_reply": "2023-10-27T21:55:41.631107Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "yhat = ...\n",
    "lasso_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6ca60",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812bb4cd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a390aad8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c0f72",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q10": {
     "name": "q10",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> cstar==5\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(acc_star,0.9692546764261647,1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q11": {
     "name": "q11",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(best_features)=={'mean concave points',\n...  'radius error',\n...  'worst concave points',\n...  'worst radius',\n...  'worst smoothness',\n...  'worst symmetry',\n...  'worst texture'}\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> max_theta_feature=='worst radius'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.all(np.isclose(lasso_model.named_steps['logreg'].coef_[0,:],[0.        , 0.        , 0.        , 0.        , 0.        ,\n...        0.        , 0.        , 0.73708439, 0.        , 0.        ,\n...        0.49742808, 0.        , 0.        , 0.        , 0.        ,\n...        0.        , 0.        , 0.        , 0.        , 0.        ,\n...        2.00925427, 0.7288694 , 0.        , 0.        , 0.21378322,\n...        0.        , 0.        , 0.87670019, 0.08482143, 0.        ],1e-3))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q12": {
     "name": "q12",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(lasso_test,0.9912280701754386,1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 3
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> Xtrain.shape==(455, 30)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> Xtest.shape==(114, 30)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> Xtrain_norm.shape==(455, 30)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> np.all(np.isclose(Xtrain_norm.mean(),0,1e-4))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> best_single_feature=='worst concave points'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.all(np.isclose(model_norm.intercept_[0],-1.0850505229077547,1e-3))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.all(np.isclose(model_norm.coef_[0,:],[3.44438434],1e-3))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.all(np.isclose(pipeline.named_steps['scaler'].scale_,[0.06523992],1e-3))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(pipeline.named_steps['logreg'].intercept_[0], -1.0850505229077547, 1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.all(np.isclose(pipeline.named_steps['logreg'].coef_[0,:], [3.44438434], 1e-3))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(acc_nonorm,0.7186882769838503,1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(acc_norm,0.9077785523411176,1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(acc_norm,0.9077785523411176,1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(acc_nonorm_test,0.7894736842105263,1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> np.isclose(acc_pipe_test,0.9210526315789473,1e-3)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9": {
     "name": "q9",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.all(np.isclose(acc,[0.89679912, 0.92315848, 0.92755896, 0.94514639, 0.95827524,\n...        0.96925468, 0.96706169, 0.96266121, 0.96709074, 0.96488323,\n...        0.96488323, 0.96269025, 0.96267573, 0.96267573, 0.95827524,\n...        0.95609678, 0.95171082, 0.94950331, 0.94948879, 0.94729581],1e-3))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
